{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7b3dd5",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron\n",
    "\n",
    "The implementation of a multi-layer perceptron for the purposes of sentiment analysis. \n",
    "\n",
    "## Authors\n",
    "- Matthew Freestone\n",
    "- Will Humphlett\n",
    "- Matthew Shipplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88acdd",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Configures the notebook, assumes you are using the base conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.bootstrap import setup\n",
    "\n",
    "setup()\n",
    "    \n",
    "print(\"Notebook setup has completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4455f12",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Downloads configured dataset and performs necessary environment bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.bootstrap import *\n",
    "\n",
    "# other datasets available in src.bootstrap\n",
    "DATASETS = [CDS_AND_VINYL_JSON_PARAMS, CELL_PHONE_JSON_PARAMS , KINDLE_STORE_JSON_PARAMS, SPORTS_JSON_PARAMS]\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    download_data_file(dataset['file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de69ce6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Sanitize data to prepare for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34451556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import get_dataframe\n",
    "\n",
    "df = get_dataframe(DATASETS, equalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbef2d4",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Convert the dataset into a usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f44ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from src.preprocessing import get_subsets\n",
    "\n",
    "word_vectorizer = CountVectorizer(\n",
    "    min_df=0.0001, \n",
    "    max_df=0.7\n",
    ")\n",
    "\n",
    "X = word_vectorizer.fit_transform(df[\"reviewText\"].to_numpy())\n",
    "\n",
    "# optional, convert a 5 class problem into a 3 class problem\n",
    "df['overall'].replace(2, 1, inplace=True)\n",
    "df['overall'].replace(4, 5, inplace=True)\n",
    "\n",
    "y = df['overall'].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_subsets(X,y, train_split=0.8, val_split=0.1, test_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0711f",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Full implementation of our Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Sequence, Tuple\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"overflow encountered in exp\")\n",
    "\n",
    "\n",
    "def sigmoid(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "\n",
    "def dSigmoid(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    a = 1.0 / (1.0 + np.exp(-X))\n",
    "    return a * (1 - a)\n",
    "\n",
    "\n",
    "def tanh(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Tanh function\"\"\"\n",
    "    return np.tanh(X)\n",
    "\n",
    "\n",
    "def dTanh(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of tanh function\"\"\"\n",
    "    return 1.0 - np.tanh(X) ** 2\n",
    "\n",
    "\n",
    "def relu(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Rectified linear unit function\"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "\n",
    "def dRelu(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Derivative of rectified linear unit function\"\"\"\n",
    "    return np.where(X > 0, 1, 0)\n",
    "\n",
    "\n",
    "def softmax(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Softmax\"\"\"\n",
    "    tmp = X - X.max(axis=1)[:, np.newaxis]\n",
    "    return np.exp(tmp) / np.exp(tmp).sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "def squared_loss(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"Mean squared loss\"\"\"\n",
    "    return ((y_true - y_pred) ** 2).mean() / 2\n",
    "\n",
    "\n",
    "def L1_reg_loss(weights: List[np.ndarray]) -> float:\n",
    "    \"\"\"Lasso regression\"\"\"\n",
    "    c = 0\n",
    "    for w in weights:\n",
    "        c += np.sum(np.abs(w))\n",
    "    return c\n",
    "\n",
    "\n",
    "def L1_reg_grad(weights: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"Lasso regression grad\"\"\"\n",
    "    grad = []\n",
    "    for w in weights:\n",
    "        grad.append(np.where(w > 0, 1, -1))\n",
    "    return grad\n",
    "\n",
    "\n",
    "def L2_reg_loss(weights: List[np.ndarray]) -> float:\n",
    "    \"\"\"Ridge regression\"\"\"\n",
    "    c = 0\n",
    "    for w in weights:\n",
    "        w = w.ravel()\n",
    "        c += np.dot(w, w)\n",
    "    return c\n",
    "\n",
    "\n",
    "def L2_reg_grad(weights: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"Ridge regression grad\"\"\"\n",
    "    return [2 * w for w in weights]\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        lr: float,\n",
    "        hidden_layers: Sequence[int],\n",
    "        regularization: str = None,\n",
    "        reg_const: float = 0.0,\n",
    "        activation: str = \"sigmoid\",\n",
    "    ):\n",
    "        \"\"\"An implementation of a multi-layer perceptron with backpropagation\n",
    "\n",
    "        :param epochs: Number of epochs\n",
    "        :param lr: Learning rate\n",
    "        :param hidden_layers: Sequence of integers representing the number of neurons in each hidden layer\n",
    "        :param regularization: Regularization function, [\"l1\", \"l2\"]\n",
    "        :param reg_const: Regularization constant\n",
    "        :param activation: Activation function, [\"sigmoid\", \"tanh\", \"relu\"], default=\"sigmoid\"\n",
    "        \"\"\"\n",
    "        self.num_epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.regularization = regularization\n",
    "        self.reg_const = reg_const\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_layer = None\n",
    "        self.input_layer = None\n",
    "        self._num_layers = len(self.hidden_layers) + 2\n",
    "        self._yencoder = LabelBinarizer()\n",
    "\n",
    "        self._output_activation = None\n",
    "\n",
    "        if activation == \"sigmoid\" or activation == \"logistic\":\n",
    "            self.activation = sigmoid\n",
    "            self.dActivation = dSigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "            self.dActivation = dTanh\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = relu\n",
    "            self.dActivation = dRelu\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        if regularization is None:\n",
    "            pass\n",
    "        elif regularization.lower() == \"l1\":\n",
    "            self._loss_reg = L1_reg_loss\n",
    "            self._grad_reg = L1_reg_grad\n",
    "        elif regularization.lower() == \"l2\":\n",
    "            self._loss_reg = L2_reg_loss\n",
    "            self._grad_reg = L2_reg_grad\n",
    "        else:\n",
    "            raise ValueError(\"Invalid regularization function\")\n",
    "\n",
    "        self._loss_function = squared_loss\n",
    "        self._biases = None\n",
    "        self._weights = None\n",
    "        self.train_loss_curve = []\n",
    "        self.val_loss_curve = []\n",
    "\n",
    "    def epochs(self):\n",
    "        for i in range(self.num_epochs):\n",
    "            yield i, self.lr\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        X_val: np.ndarray = None,\n",
    "        y_val: np.ndarray = None,\n",
    "        batch_size: int = 1,\n",
    "        continue_fit=False,\n",
    "        output=True,\n",
    "    ) -> None:\n",
    "        \"\"\"Fits the model to given data\n",
    "\n",
    "        :param X: Input data of shape (n_examples, n_features)\n",
    "        :param y: Output data of shape (n_examples, )\n",
    "        :param X_val: Validation input data of shape (n_examples, n_features)\n",
    "        :param y_val: Validation output data of shape (n_examples, )\n",
    "        :param batch_size: Size of the batch to be used for training, default=1\n",
    "        :param continue_fit: Continue training from last epoch, default=False\n",
    "        :param output: Display a progress bar of model fitting, default=True\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise ValueError(\"Invalid shape for X\")\n",
    "        if len(y.shape) != 1:\n",
    "            raise ValueError(\"Invalid shape for y\")\n",
    "        n_examples, n_features = X.shape\n",
    "\n",
    "        use_val = False\n",
    "        if X_val is not None and y_val is not None:\n",
    "            use_val = True\n",
    "            if len(X_val.shape) != 2:\n",
    "                raise ValueError(\"Invalid shape for X_val\")\n",
    "            if len(y_val.shape) != 1:\n",
    "                raise ValueError(\"Invalid shape for y_val\")\n",
    "        y_combined = np.concatenate((y, y_val)) if use_val else y\n",
    "        y_combined = self._format_labels(y_combined)\n",
    "        y, y_val = np.split(y_combined, [n_examples]) if use_val else (y_combined, None)\n",
    "\n",
    "        if not continue_fit:\n",
    "            self.input_layer = n_features\n",
    "            self.output_layer = y.shape[1]\n",
    "            self._structure = (self.input_layer, *self.hidden_layers, self.output_layer)\n",
    "\n",
    "            self._biases = [np.random.randn(y, 1) for y in self._structure[1:]]\n",
    "            self._weights = [np.random.randn(x, y) for x, y in zip(self._structure[:-1], self._structure[1:])]\n",
    "\n",
    "        epochs = tqdm(self.epochs(), total=self.num_epochs) if output else self.epochs()\n",
    "\n",
    "        for epoch_num, lr in epochs:\n",
    "            train_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i : i + batch_size]\n",
    "                y_batch = y[i : i + batch_size]\n",
    "                dJdB, dJdW, c_loss = self._backprop(X_batch, y_batch)\n",
    "                train_loss += c_loss\n",
    "\n",
    "                self._biases = [b - lr * db for b, db in zip(self._biases, dJdB)]\n",
    "                self._weights = [w - lr * dw for w, dw in zip(self._weights, dJdW)]\n",
    "            num_batches = X.shape[0] // batch_size\n",
    "            self.train_loss_curve.append(train_loss / num_batches)\n",
    "            if use_val:\n",
    "                val_loss = self._calc_loss(y_val, self._fast_forward_pass(X_val))\n",
    "                self.val_loss_curve.append(val_loss)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predicts the output for the given input\n",
    "\n",
    "        :param X: Input data of shape (n_examples, n_features)\n",
    "        :return: Output data of shape (n_examples, )\n",
    "        \"\"\"\n",
    "        curr = self._fast_forward_pass(X)\n",
    "        if self.output_layer == 1:\n",
    "            curr = curr.ravel()\n",
    "        return self._yencoder.inverse_transform(curr)\n",
    "\n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Returns the accuracy of the model on the given data\n",
    "\n",
    "        :param X: Input data of shape (n_examples, n_features)\n",
    "        :param y: Output data of shape (n_examples, )\n",
    "        :return: Accuracy score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "    def plot_loss(self) -> None:\n",
    "        \"\"\"The loss over epochs plot\n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        plt.plot(self.train_loss_curve, label=\"Training loss\")\n",
    "        if self.val_loss_curve and len(self.val_loss_curve) == len(self.train_loss_curve):\n",
    "            plt.plot(self.val_loss_curve, label=\"Validation loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss vs Epoch\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def _format_labels(self, y: np.ndarray) -> np.ndarray:\n",
    "        if len(y.shape) == 2 and y.shape[1] == 1:\n",
    "            y = y.ravel()\n",
    "        elif len(y.shape) == 2 and y.shape[1] > 1 or len(y.shape) > 2:\n",
    "            raise ValueError(\"Invalid shape for y\")\n",
    "\n",
    "        self._yencoder.fit(y)\n",
    "        if len(self._yencoder.classes_) == 2:\n",
    "            self._output_activation = sigmoid\n",
    "        else:\n",
    "            self._output_activation = softmax\n",
    "        return self._yencoder.transform(y)\n",
    "\n",
    "    def _backprop(self, X: np.ndarray, y: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray], float]:\n",
    "        # initialize empty lists to store the gradient of the biases and weights\n",
    "        dBias = [np.zeros(b.shape) for b in self._biases]\n",
    "        dWeights = [np.zeros(w.shape) for w in self._weights]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # do a forward pass to get the activations and z values\n",
    "        layer_raw = []  # stores the weighted sum of inputs for each layer\n",
    "        layer_activations = []  # stores the output of each layer\n",
    "        a = X  # input layer\n",
    "        for i, (b, W) in enumerate(zip(self._biases, self._weights)):\n",
    "            # compute the weighted sum of inputs for this layer\n",
    "            z = a @ W + b.T\n",
    "\n",
    "            # apply the activation function to the output of this layer\n",
    "            if i < self._num_layers - 2:\n",
    "                a = self.activation(z)\n",
    "            else:\n",
    "                a = self._output_activation(z)\n",
    "            # store the raw output and the activated output of this layer\n",
    "            layer_raw.append(z)\n",
    "            layer_activations.append(a)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self._calc_loss(y, a)\n",
    "\n",
    "        # index of the last hidden layer\n",
    "        last_hidden = self._num_layers - 2\n",
    "\n",
    "        # compute the error at the last hidden layer\n",
    "        delta = layer_activations[last_hidden] - y\n",
    "\n",
    "        # compute the gradient of the biases and weights for the last hidden layer\n",
    "        dBias[last_hidden] = np.mean(delta, axis=0)\n",
    "        dWeights[last_hidden] = layer_activations[last_hidden - 1].T @ delta\n",
    "\n",
    "        # for all hidden layers except the first and last,\n",
    "        # compute the gradient of the biases and weights\n",
    "        for L in range(last_hidden - 1, 0, -1):\n",
    "            # compute the error at this layer\n",
    "            delta = (delta @ self._weights[L + 1].T) * self.dActivation(layer_raw[L])\n",
    "\n",
    "            # compute the gradient of the biases and weights for this layer\n",
    "            dBias[L] = np.mean(delta, axis=0)\n",
    "            dWeights[L] = layer_activations[L - 1].T @ delta\n",
    "\n",
    "        # for the input layer, compute the gradient of the biases and weights\n",
    "        # using the inputs, rather than the previous layer\n",
    "        delta = (delta @ self._weights[1].T) * self.dActivation(layer_raw[0])\n",
    "        dBias[0] = np.mean(delta, axis=0)\n",
    "        dWeights[0] = X.T @ delta\n",
    "\n",
    "        # add a second dimension to the bias gradient to make it compatible with the bias shape\n",
    "        dBias = [db[:, np.newaxis] for db in dBias]\n",
    "        # divide by number of samples to get the average gradient\n",
    "        dWeights = [dw / n_samples for dw in dWeights]\n",
    "\n",
    "        # apply regularization if enabled\n",
    "        if self.regularization:\n",
    "            dWeights = [dw + self.reg_const * r for dw, r in zip(dWeights, self._grad_reg(self._weights))]\n",
    "        return dBias, dWeights, loss\n",
    "\n",
    "    def _calc_loss(self, y_pred: np.ndarray, y_batch: np.ndarray) -> float:\n",
    "        loss = self._loss_function(y_pred, y_batch)\n",
    "        if self.regularization:\n",
    "            loss += (0.5 * self.reg_const) * self._loss_reg(self._weights) / y_pred.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def _fast_forward_pass(self, X: np.ndarray) -> np.ndarray:\n",
    "        curr = X\n",
    "        for i in range(self._num_layers - 1):\n",
    "            curr = curr @ self._weights[i]\n",
    "            curr += self._biases[i].T\n",
    "            if i < self._num_layers - 2:\n",
    "                curr = self.activation(curr)\n",
    "            else:\n",
    "                curr = self._output_activation(curr)\n",
    "        return curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe832a0e",
   "metadata": {},
   "source": [
    "## Two Dimension Problem\n",
    "Prove that our implemenation of an MLP can learn non-linear decision planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.two_dim_problem import TwoDimProblem\n",
    "\n",
    "problem = TwoDimProblem(5)\n",
    "X, y = problem.create_data(2, 0.05, 500)\n",
    "problem.plot_data(show_seperator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1588a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(epochs=50, lr=0.2, hidden_layers=(10, 5))\n",
    "mlp.fit(X, y)\n",
    "problem.plot_pred(mlp.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5e96b",
   "metadata": {},
   "source": [
    "## Training\n",
    "Determine optimal hyperparamters given our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c81192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import matrix_train\n",
    "\n",
    "# each axis must be an iterable. if you want to use a constant, wrap it in an iterable of len 1\n",
    "hyperparameter_matrix = {\n",
    "    \"epochs\": np.logspace(np.log10(10), np.log10(100), num=5, dtype=\"int64\"),\n",
    "    \"lr\": np.logspace(np.log10(.00002), np.log10(.2), num=5),\n",
    "    \"hidden_layers\": [[50, 20], [50, 50, 20, 20], [100, 40]],\n",
    "    \"activation\": [\"sigmoid\", \"tanh\", \"relu\"],\n",
    "    \"regularization\": [\"L2\", \"L1\"],\n",
    "    \"reg_const\": [0.0001, 0.001],\n",
    "}\n",
    "\n",
    "# look at data/train.log for progress\n",
    "\n",
    "best_params = matrix_train(hyperparameter_matrix, MultiLayerPerceptron, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(best_params)\n",
    "# mlp = MultiLayerPerceptron(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(**best_params)\n",
    "mlp.fit(X_train, y_train, X_val, y_val, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7486d",
   "metadata": {},
   "source": [
    "## Performance\n",
    "Detailed performance report of our mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis import accuracy, confusion, report, plots\n",
    "\n",
    "print(report(mlp.predict(X_test), y_test))\n",
    "\n",
    "plots(mlp.score(X_train, y_train), mlp.score(X_test, y_test), mlp.score(X_val, y_val), mlp.train_loss_curve, mlp.val_loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112831b",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3ad38",
   "metadata": {},
   "source": [
    "### SKLearn Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0bff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "skl_mlp = MLPClassifier(\n",
    "    early_stopping=True,\n",
    ")\n",
    "skl_mlp = skl_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc540728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(skl_mlp.predict(X_test), y_test))\n",
    "\n",
    "plots(skl_mlp.score(X_train, y_train), skl_mlp.score(X_test, y_test), skl_mlp.score(X_val, y_val), skl_mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce3ed9",
   "metadata": {},
   "source": [
    "### SKLearn Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef60d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(rf.predict(X_test), y_test))\n",
    "\n",
    "plots(rf.score(X_train, y_train), rf.score(X_test, y_test), rf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8c47d",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dbc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# clear tensoflow backend\n",
    "tf.keras.backend.clear_session()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(20, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\"accuracy\", \"mse\", \"mae\"]\n",
    ")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05966ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_X_train = X_train.toarray()\n",
    "keras_X_val = X_val.toarray()\n",
    "keras_y_train = y_train - 1\n",
    "keras_y_val = y_val - 1\n",
    "model.fit(keras_X_train, keras_y_train, epochs=10, validation_data=(keras_X_val, keras_y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b045c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
