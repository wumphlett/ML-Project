{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de3eddd",
   "metadata": {},
   "source": [
    "# Notebook Setup\n",
    "Installs necessary requirements and configures proper notebook behavior  \n",
    "Note: The notebook assumes you are using the base conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "print(\"Notebook setup has completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d528420",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b600f8",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load the data from the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDS_AND_VINYL_JSON_PARAMS = {\n",
    "    'file': 'reviews_CDs_and_Vinyl_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "CELL_PHONE_JSON_PARAMS = {\n",
    "    'file': 'reviews_Cell_Phones_and_Accessories_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "CLOTHING_JSON_PARAMS = {\n",
    "    'file': 'reviews_Clothing_Shoes_and_Jewelry_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "ELECTRONICS_JSON_PARAMS = {\n",
    "    'file': 'reviews_Electronics_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "HOME_AND_KITCHEN_JSON_PARAMS = {\n",
    "    'file': 'reviews_Home_and_Kitchen_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "KINDLE_STORE_JSON_PARAMS = {\n",
    "    'file': 'reviews_Kindle_Store_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "MOVIES_JSON_PARAMS = {\n",
    "    'file': 'reviews_Movies_and_TV_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}\n",
    "SPORTS_JSON_PARAMS = {\n",
    "    'file': 'reviews_Sports_and_Outdoors_5.json',\n",
    "    'filetype': 'json',\n",
    "    'features': \"reviewText\",\n",
    "    'labels': \"overall\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please add the above files you have in your data directory\n",
    "files = [ELECTRONICS_JSON_PARAMS, CDS_AND_VINYL_JSON_PARAMS, CELL_PHONE_JSON_PARAMS, HOME_AND_KITCHEN_JSON_PARAMS, SPORTS_JSON_PARAMS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import get_dataframe_file\n",
    "frames = []\n",
    "for file in files:\n",
    "    frame = get_dataframe_file(params = file, points=50_000, equalize=True)\n",
    "    frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat(frames)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc59bc",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "Split into training, testing, and validation sets, and vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from preprocessing import get_subsets\n",
    "\n",
    "word_vectorizer = CountVectorizer(\n",
    "    min_df=0.0001, \n",
    "    max_df=0.7\n",
    " )\n",
    "X = df[\"reviewText\"].to_numpy()\n",
    "X = word_vectorizer.fit_transform(X)\n",
    "y = df['overall'].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_subsets(X,y, train_split=0.8, val_split=0.1, test_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370a703",
   "metadata": {},
   "source": [
    "# Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.bar(np.unique(y_train, return_counts=True)[0], np.unique(y_train, return_counts=True)[1])\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data and shapes:\")\n",
    "print(\"X_train:\", type(X_train), \"of shape\", X_train.shape)\n",
    "print(\"Y_train:\", type(y_train), \"of shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11c14f",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MultiLayerPerceptron\n",
    "clf = MultiLayerPerceptron(\n",
    "    epochs=50,\n",
    "    lr=0.1,\n",
    "    hidden_layers=[500, 200],\n",
    "    activation=\"sigmoid\",\n",
    ")\n",
    "clf.fit(X_train, y_train, X_val, y_val, batch_size=100)\n",
    "clf.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e634ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"Testing accuracy: {clf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = classification_report(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932bfcf",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03803e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(500, 200),\n",
    "    activation=\"logistic\",\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    ")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = clf.predict(X_test)\n",
    "print(f\"Training accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"Testing accuracy: {clf.score(X_test, y_test)}\")\n",
    "report = classification_report(y_test, pred)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=50,\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fce156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = clf.predict(X_test)\n",
    "print(f\"Training accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"Testing accuracy: {clf.score(X_test, y_test)}\")\n",
    "report = classification_report(y_test, pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1624a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = clf.predict_proba(X_test)\n",
    "p = p @ np.array([-1,1]).reshape(2,1)\n",
    "print(f\"Most positive example is {np.argmax(p)}\")\n",
    "print(f\"Most negative example is {np.argmin(p)}\")\n",
    "print(f\"Most neutral example is {np.argmin(np.abs(p))}\")\n",
    "\n",
    "p = (p*0.5 + 0.5)\n",
    "incorrect_amount = p - y_test.reshape(-1,1)\n",
    "print(f'Most incorrect example is {np.argmax(np.abs(incorrect_amount))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f119f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 6197\n",
    "print(f\"Predicted: {best_clf.predict(X_test[example])[0]} {p[example]}\\nActual: {y_test[example]}\")\n",
    "print(word_vectorizer.inverse_transform(X_test[example])[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f74f92",
   "metadata": {},
   "source": [
    "## Runs\n",
    "1. 89.98: (10,5), Logisitic, Early Stopping\n",
    "2. 89.95: (10,5), Relu, Early Stopping,\n",
    "3. 89.71: (5), Relu, Early Stopping\n",
    "4. 85.12: (5), Relu, \n",
    "5. 91.11: (5), Relu, Early Stopping, 2-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08a055",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Lambda, Symbol\n",
    "from training import matrix_train\n",
    "\n",
    "\n",
    "x = Symbol(\"x\")\n",
    "\n",
    "\n",
    "# each axis must be an iterable. if you want to use a constant, wrap it in an iterable of len 1\n",
    "hyperparameter_matrix = {\n",
    "    \"epochs\": np.logspace(np.log10(100), np.log10(100000), num=20, dtype=\"int64\"),\n",
    "    \"lr\": np.logspace(np.log10(.00001), np.log10(.1), num=20),\n",
    "    \"hidden_layers\": [5, 6, 7],\n",
    "    \"neurons_per_layer\": [3],\n",
    "    \"activation\": [Lambda(x, x**2)],\n",
    "}\n",
    "\n",
    "best_params = matrix_train(hyperparameter_matrix, MultiLayerPerceptron, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(best_params)\n",
    "# mlp = MultiLayerPerceptron(**best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67082690",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ef1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import accuracy, confusion, report\n",
    "\n",
    "\n",
    "mlp = MultiLayerPerceptron(**best_params)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# print(accuracy(mlp.predict(X_test), y_test))\n",
    "# print(confusion(mlp.predict(X_test), y_test))\n",
    "print(report(mlp.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7f405",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO utilize other classifiers and compare performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "eaf02dffba55afcb757f8661e47ea199414fefb6fedafc1622a26ab926d7da3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
